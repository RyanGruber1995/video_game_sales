# Table of Contents
1. [Data Normalization](#DataNormalization)

# Data Normalization

### Overview

Although this dataset came from a public source and was mostly in a format that was usable, I took this opportunity to explore the data and discovered that the table could be optimized. The following is a screenshot of the columns and the first few rows of the raw data set:

<br />

![](https://github.com/RyanGruber1995/video_game_sales/blob/main/screenshots/data_normalization.PNG)

<br />

I identified three different attributes that could be broken out into different tables: Platform, Genre, and Publisher. These columns contain string data and can have multiple of the same value within their respective columns, e.g. having 'Nintendo' multiple times per the screenshot. Even though this particular dataset is able to fit in a spreadsheet, this would not be an ideal situation for a much larger dataset and processing time for running SQL queries would be reduced. Using Google Sheets, I created these three new tables, created ids for each table (primary keys), and mapped the ids to the games table which was created by taking a copy of the raw data and adding in new columns to map the ids to the new tables. The process for creating the platform, genre, and publisher tables were the same, so I will walkthrough the process of creating the platform table which was then applied to the other tables.

### Creating Tables

I created two columns for the platform table: *platform_id* and *platform_name*. *platform_name* was generated by taking the unique values of the platform column from the raw data, and although unnecssary, I sorted the values alphabetically. *platform_id* was generated by starting at 1 and incrementing the vlue by 1 for each row. Below is a screenshot of the first few rows of the platform table along with the formula used to generate *platform_name*:

<br />

![](https://github.com/RyanGruber1995/video_game_sales/blob/main/screenshots/secondary_table_creation.PNG)

<br />

This created the first inconsistency by separating data because '2600' was formatted as a number, and I converted the entire column in both the platform and games tables to ensure the same data type.

<br />

Once these tables were created, it was time to finalize making the games table. To do this, I mapped the record for each of these three attributes to their respective tables by using an INDEX/MATCH function. Below is a screenshot of the formula and mapping process used for the *platform_id* with the same methodology being applied to the other id columns:

<br />

![](https://github.com/RyanGruber1995/video_game_sales/blob/main/screenshots/id_creation.PNG)

<br />

Because I wanted to remove the platform, genre, and publisher columns, and their respective id columns were referencing these columns, I needed to hardcode everything so those columns could be removed without causing errors. The [vgsales_draft.xlsx](https://github.com/RyanGruber1995/video_game_sales/blob/main/vgsales_draft.xlsx) file keeps track of the formulas I created before hardcoding and removing columns while the [vgsales_final.xlsx](https://github.com/RyanGruber1995/video_game_sales/blob/main/vgsales_final.xlsx) file shows the finalized tables that were uploaded into BigQuery for further analysis.

<br />

# BigQuery

### Data Exploration and cleaning

I initially took some time to explore the data and confirm that the data was ready for use. 
