# Table of Contents
1. [Data Normalization](#DataNormalization)

# Data Normalization

### Overview

Although this dataset came from a public source and was mostly in a format that was usable, I took this opportunity to explore the data and discovered that the table could be optimized. The following is a screenshot of the columns and the first few rows of the raw data set:

<br />

![](https://github.com/RyanGruber1995/video_game_sales/blob/main/screenshots/data_normalization.PNG)

<br />

I identified three different attributes that could be broken out into different tables: Platform, Genre, and Publisher. These columns contain string data and can have multiple of the same value within their respective columns, e.g. having 'Nintendo' multiple times per the screenshot. Even though this particular dataset is able to fit in a spreadsheet, this would not be an ideal situation for a much larger dataset and processing time for running SQL queries would be reduced. Using Google Sheets, I created these three new tables, created ids for each table (primary keys), and mapped the ids to the games table which was created by taking a copy of the raw data and adding in new columns to map the ids to the new tables. The process for creating the platform, genre, and publisher tables were the same, so I will walkthrough the process of creating the platform table which was then applied to the other tables.

### Creating Tables

I created two columns for the platform table: *platform_id* and *platform_name*. *platform_name* was generated by taking the unique values of the platform column from the raw data, and although unnecssary, I sorted the values alphabetically. *platform_id* was generated by starting at 1 and incrementing the vlue by 1 for each row. Below is a screenshot of the first few rows of the platform table along with the formula used to generate *platform_name*:

<br />

![](https://github.com/RyanGruber1995/video_game_sales/blob/main/screenshots/secondary_table_creation.PNG)

<br />

This created the first inconsistency by separating data because '2600' was formatted as a number, and I converted the entire column in both the platform and games tables to ensure the same data type.

<br />

Once these tables were created, it was time to finalize making the games table. To do this, I mapped the record for each of these three attributes to their respective tables by using an INDEX/MATCH function. Below is a screenshot of the formula and mapping process used for the *platform_id* with the same methodology being applied to the other id columns:

<br />

![](https://github.com/RyanGruber1995/video_game_sales/blob/main/screenshots/id_creation.PNG)

<br />

Because I wanted to remove the platform, genre, and publisher columns, and their respective id columns were referencing these columns, I needed to hardcode everything so those columns could be removed without causing errors. The [vgsales_draft.xlsx](https://github.com/RyanGruber1995/video_game_sales/blob/main/vgsales_draft.xlsx) file keeps track of the formulas I created before hardcoding and removing columns while the [vgsales_final.xlsx](https://github.com/RyanGruber1995/video_game_sales/blob/main/vgsales_final.xlsx) file shows the finalized tables that were uploaded into BigQuery for further analysis.

<br />

# BigQuery

### Data Exploration and cleaning

The first thing I did was create a dataset called vgsales as well as tables for the four tables that were created during the data normalization process as shown here:

<br />

![](https://github.com/RyanGruber1995/video_game_sales/blob/main/screenshots/BigQuery_tables.PNG)

<br />

I initially took some time to explore the data and confirm that the data was ready for use. The first issue I came across was the schema of the games table as the data type of the year column was a string. Upon further analysis I noticed that some games had a year of 'N/A' as either the year was probably unknown of just not entered. By running the following code, I discovered that 271 games had this issue. It wouldn't be hard to fix this by researching when those games were released and updating them to have the complete dataset, but as there are so many to account for I decided to remove those observations.

    # Count number of rows that have year as 'N/A'
    SELECT COUNT(*)
    FROM vgsales.games
    WHERE year = "N/A"

    # Delete rows where year is unknown
    DELETE FROM vgsales.games WHERE year = "N/A"

I know from the site where I pulled this dataset that the data tracked games since 1980, but not necessary what the most recent year was. To find this out I ran the following query:

    # Count games from each year
    SELECT 
      year,
      COUNT(*) AS num_of_games
    FROM vgsales.games
    GROUP BY year
    ORDER BY year ASC

| year 	| num_of_games |
| :---  | :--- |
1980	| 9    
1981	| 46
1982	| 36
1983	| 17
1984	| 14
1985	| 14
1986	| 21
1987	| 16
1988	| 15
1989	| 17
1990	| 16
1991	| 41
1992	| 43
1993	| 60
1994	| 121
1995	| 219
1996	| 263
1997	| 289
1998	| 379
1999	| 338
2000	| 349
2001	| 482
2002	| 829
2003	| 775
2004	| 763
2005	| 941
2006	| 1008
2007	| 1202
2008	| 1428
2009	| 1431
2010	| 1259
2011	| 1139
2012	| 657
2013	| 546
2014	| 582
2015	| 614
2016	| 344
2017	| 3
2020	| 1
