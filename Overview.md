# Table of Contents
1. [Data Normalization](#DataNormalization)

# Data Normalization

Although this dataset came from a public source and was mostly in a format that was usable, I took this opportunity to explore the data and discovered that the table could be optimized. The following is a screenshot of the columns and the first few rows of the raw data set:

<br />

![](https://github.com/RyanGruber1995/video_game_sales/blob/main/screenshots/data_normalization.PNG)

<br />

I identified three different attributes that could be broken out into different tables: Platform, Genre, and Publisher. These columns contain string data and can have multiple of the same value within their respective columns, e.g. having 'Nintendo' multiple times per the screenshot. Even though this particular dataset is able to fit in a spreadsheet, this would not be an ideal situation for a much larger dataset and processing time for running SQL queries would be reduced. Using Google Sheets, I created these three new tables, created ids for each table (primary keys), and mapped the ids to the games table which was created by taking a copy of the raw data and adding in new columns to map the ids to the new tables. The process for creating the platform, genre, and publisher tables were the same, so I will walkthrough the process of creating the platform table which was then applied to the other tables.

<br />

I created two columns for the platform table: *platform_id* and *platform_name*. *platform_name* was generated by taking the unique values of the platform column from the raw data, and although unnecssary, I sorted the values alphabetically. *platform_id* was generated by starting at 1 and incrementing the vlue by 1 for each row. Below is a screenshot of the first few rows of the platform table along with the formula used to generate *platform_name*:

<br />

![](https://github.com/RyanGruber1995/video_game_sales/blob/main/screenshots/secondary_table_creation.PNG)

<br />
