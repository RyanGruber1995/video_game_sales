# Table of Contents
1. [Data Normalization](#DataNormalization)

# Data Normalization

### Overview

Although this dataset came from a public source and was mostly in a format that was usable, I took this opportunity to explore the data and discovered that the table could be optimized. The following is a screenshot of the columns and the first few rows of the raw data set:

<br />

![](https://github.com/RyanGruber1995/video_game_sales/blob/main/screenshots/data_normalization.PNG)

<br />

I identified three different attributes that could be broken out into different tables: Platform, Genre, and Publisher. These columns contain string data and can have multiple of the same value within their respective columns, e.g. having 'Nintendo' multiple times per the screenshot. Even though this particular dataset is able to fit in a spreadsheet, this would not be an ideal situation for a much larger dataset and processing time for running SQL queries would be reduced. Using Google Sheets, I created these three new tables, created ids for each table (primary keys), and mapped the ids to the games table which was created by taking a copy of the raw data and adding in new columns to map the ids to the new tables. The process for creating the platform, genre, and publisher tables were the same, so I will walkthrough the process of creating the platform table which was then applied to the other tables.

### Creating Tables

I created two columns for the platform table: *platform_id* and *platform_name*. *platform_name* was generated by taking the unique values of the platform column from the raw data, and although unnecssary, I sorted the values alphabetically. *platform_id* was generated by starting at 1 and incrementing the vlue by 1 for each row. Below is a screenshot of the first few rows of the platform table along with the formula used to generate *platform_name*:

<br />

![](https://github.com/RyanGruber1995/video_game_sales/blob/main/screenshots/secondary_table_creation.PNG)

<br />

This created the first inconsistency by separating data because '2600' was formatted as a number, and I converted the entire column in both the platform and games tables to ensure the same data type.

<br />

Once these tables were created, it was time to finalize making the games table. To do this, I mapped the record for each of these three attributes to their respective tables by using an INDEX/MATCH function. Below is a screenshot of the formula and mapping process used for the *platform_id* with the same methodology being applied to the other id columns:

<br />

![](https://github.com/RyanGruber1995/video_game_sales/blob/main/screenshots/id_creation.PNG)

<br />

Because I wanted to remove the platform, genre, and publisher columns, and their respective id columns were referencing these columns, I needed to hardcode everything so those columns could be removed without causing errors. The [vgsales_draft.xlsx](https://github.com/RyanGruber1995/video_game_sales/blob/main/vgsales_draft.xlsx) file keeps track of the formulas I created before hardcoding and removing columns while the [vgsales_final.xlsx](https://github.com/RyanGruber1995/video_game_sales/blob/main/vgsales_final.xlsx) file shows the finalized tables that were uploaded into BigQuery for further analysis.

<br />

# BigQuery

### Data Exploration and cleaning

The first thing I did was create a dataset called vgsales as well as tables for the four tables that were created during the data normalization process as shown here:

<br />

![](https://github.com/RyanGruber1995/video_game_sales/blob/main/screenshots/BigQuery_tables.PNG)

<br />

I initially took some time to explore the data and confirm that the data was ready for use. The first issue I came across was the schema of the games table as the data type of the year column was a string. Upon further analysis I noticed that some games had a year of 'N/A' as either the year was probably unknown of just not entered. By running the following code, I discovered that 271 games had this issue. It wouldn't be hard to fix this by researching when those games were released and updating them to have the complete dataset, but as there are so many to account for I decided to remove those observations.

    # Count number of rows that have year as 'N/A'
    SELECT COUNT(*)
    FROM vgsales.games
    WHERE year = "N/A"

    # Delete rows where year is unknown
    DELETE FROM vgsales.games WHERE year = "N/A"

I know from the site where I pulled this dataset that the data tracked games since 1980, but not necessary what the most recent year was. To find this out I ran the following query:

    # Count games from each year
    SELECT 
      year,
      COUNT(*)
    FROM vgsales.games
    GROUP BY year
    ORDER BY year ASC

| year 	| f0_  |
| :---  | :--- |
1980	9    
1981	46
1982	36
1983	17
1984	14
1985	14
1986	21
1987	16
1988	15
1989	17
1990	16
1991	41
1992	43
1993	60
1994	121
1995	219
1996	263
1997	289
1998	379
1999	338
2000	349
2001	482
2002	829
2003	775
2004	763
2005	941
2006	1008
2007	1202
2008	1428
2009	1431
2010	1259
2011	1139
2012	657
2013	546
2014	582
2015	614
2016	344
2017	3
2020	1
